{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "**Website:** https://en.wikipedia.org/wiki/List_of_countries_and_dependencies_by_population_(United_Nations)\n",
    "\n",
    "**What I will scrape:**\n",
    "I will extract a list of all countries, including:\n",
    "- **Country name**\n",
    "- **Type** (sovereign state or dependency)\n",
    "- **Total population**\n",
    "\n",
    "**Why suitable:**\n",
    "- Fully static HTML table\n",
    "- No login or authentication required\n",
    "- Lists all countries on a single page, making it easy to scrape"
   ],
   "id": "4dbaf1b0ecd49e03"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-24T09:17:00.191245Z",
     "start_time": "2025-10-24T09:16:55.882963Z"
    }
   },
   "cell_type": "code",
   "source": [
    "!pip install requests beautifulsoup4 pandas lxml\n",
    "!pip install pandas\n"
   ],
   "id": "894add982b57f089",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: requests in /Users/ulanakrivodubskaya/anaconda3/lib/python3.11/site-packages (2.31.0)\r\n",
      "Requirement already satisfied: beautifulsoup4 in /Users/ulanakrivodubskaya/anaconda3/lib/python3.11/site-packages (4.12.2)\r\n",
      "Requirement already satisfied: pandas in /Users/ulanakrivodubskaya/anaconda3/lib/python3.11/site-packages (2.0.3)\r\n",
      "Requirement already satisfied: lxml in /Users/ulanakrivodubskaya/anaconda3/lib/python3.11/site-packages (4.9.3)\r\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/ulanakrivodubskaya/anaconda3/lib/python3.11/site-packages (from requests) (2.0.4)\r\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/ulanakrivodubskaya/anaconda3/lib/python3.11/site-packages (from requests) (3.4)\r\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/ulanakrivodubskaya/anaconda3/lib/python3.11/site-packages (from requests) (1.26.16)\r\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/ulanakrivodubskaya/anaconda3/lib/python3.11/site-packages (from requests) (2023.7.22)\r\n",
      "Requirement already satisfied: soupsieve>1.2 in /Users/ulanakrivodubskaya/anaconda3/lib/python3.11/site-packages (from beautifulsoup4) (2.4)\r\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /Users/ulanakrivodubskaya/anaconda3/lib/python3.11/site-packages (from pandas) (2.8.2)\r\n",
      "Requirement already satisfied: pytz>=2020.1 in /Users/ulanakrivodubskaya/anaconda3/lib/python3.11/site-packages (from pandas) (2023.3.post1)\r\n",
      "Requirement already satisfied: tzdata>=2022.1 in /Users/ulanakrivodubskaya/anaconda3/lib/python3.11/site-packages (from pandas) (2023.3)\r\n",
      "Requirement already satisfied: numpy>=1.21.0 in /Users/ulanakrivodubskaya/anaconda3/lib/python3.11/site-packages (from pandas) (1.24.3)\r\n",
      "Requirement already satisfied: six>=1.5 in /Users/ulanakrivodubskaya/anaconda3/lib/python3.11/site-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\r\n",
      "Requirement already satisfied: pandas in /Users/ulanakrivodubskaya/anaconda3/lib/python3.11/site-packages (2.0.3)\r\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /Users/ulanakrivodubskaya/anaconda3/lib/python3.11/site-packages (from pandas) (2.8.2)\r\n",
      "Requirement already satisfied: pytz>=2020.1 in /Users/ulanakrivodubskaya/anaconda3/lib/python3.11/site-packages (from pandas) (2023.3.post1)\r\n",
      "Requirement already satisfied: tzdata>=2022.1 in /Users/ulanakrivodubskaya/anaconda3/lib/python3.11/site-packages (from pandas) (2023.3)\r\n",
      "Requirement already satisfied: numpy>=1.21.0 in /Users/ulanakrivodubskaya/anaconda3/lib/python3.11/site-packages (from pandas) (1.24.3)\r\n",
      "Requirement already satisfied: six>=1.5 in /Users/ulanakrivodubskaya/anaconda3/lib/python3.11/site-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\r\n"
     ]
    }
   ],
   "execution_count": 74
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-24T09:17:00.210356Z",
     "start_time": "2025-10-24T09:17:00.206819Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from __future__ import annotations\n",
    "from typing import Optional, Dict, List, Tuple\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import time\n"
   ],
   "id": "df57e23f4c7bfee6",
   "outputs": [],
   "execution_count": 75
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-24T09:17:01.913481Z",
     "start_time": "2025-10-24T09:17:00.224874Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Fetcher\n",
    "# Default request headers\n",
    "\n",
    "DEFAULT_HEADERS: Dict[str, str] = {\n",
    "    \"User-Agent\": \"Mozilla/5.0 (X11; Ubuntu; Linux x86_64; rv:143.0) Gecko/20100101 Firefox/143.0\",\n",
    "    \"Accept\": \"text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8\",\n",
    "    \"Accept-Language\": \"en-GB,en;q=0.5\",\n",
    "    \"Connection\": \"keep-alive\",\n",
    "}\n",
    "\n",
    "def fetch_html(url: str, headers: Optional[Dict[str, str]] = None, timeout_s: float = 15.0) -> str:\n",
    "\n",
    "    merged_headers: Dict[str, str] = {**DEFAULT_HEADERS, **(headers or {})}\n",
    "    response = requests.get(url, headers=merged_headers, timeout=timeout_s)\n",
    "    response.raise_for_status()\n",
    "\n",
    "    # Politeness delay to avoid overwhelming the server\n",
    "    time.sleep(1.5)\n",
    "\n",
    "    return response.text\n",
    "\n",
    "# Test Fetch\n",
    "try:\n",
    "    html_preview: str = fetch_html(\"https://en.wikipedia.org/wiki/List_of_countries_and_dependencies_by_population_(United_Nations)\")\n",
    "    print(html_preview[:500])  # preview first 500 characters\n",
    "except Exception as e:\n",
    "    print(f\"Fetch failed: {e}\")\n"
   ],
   "id": "d360d8b9c18aa136",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<!DOCTYPE html>\n",
      "<html class=\"client-nojs vector-feature-language-in-header-enabled vector-feature-language-in-main-page-header-disabled vector-feature-page-tools-pinned-disabled vector-feature-toc-pinned-clientpref-1 vector-feature-main-menu-pinned-disabled vector-feature-limited-width-clientpref-1 vector-feature-limited-width-content-enabled vector-feature-custom-font-size-clientpref-1 vector-feature-appearance-pinned-clientpref-1 vector-feature-night-mode-enabled skin-theme-clientpref-day vect\n"
     ]
    }
   ],
   "execution_count": 76
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-24T09:17:03.779452Z",
     "start_time": "2025-10-24T09:17:01.948174Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Parse function\n",
    "def parse_countries_page(html: str) -> List[Dict[str, Any]]:\n",
    "\n",
    "    soup = BeautifulSoup(html, \"html.parser\")\n",
    "\n",
    "    # Find all tables with class \"wikitable\"\n",
    "    tables = soup.find_all(\"table\", class_=\"wikitable\")\n",
    "    table = None\n",
    "\n",
    "    # Select the table whose header contains \"Country\"\n",
    "    for t in tables:\n",
    "        header = t.find(\"th\")\n",
    "        if header and \"Country\" in header.get_text():\n",
    "            table = t\n",
    "            break\n",
    "\n",
    "    if not table:\n",
    "        print(\"No table found!\")\n",
    "        return []\n",
    "\n",
    "    data: List[Dict[str, Any]] = []\n",
    "\n",
    "    # Iterate over table rows, skipping the header\n",
    "    for tr in table.find_all(\"tr\")[1:]:\n",
    "        # Extract text from each column (td or th)\n",
    "        cols = [td.get_text(strip=True) for td in tr.find_all([\"td\", \"th\"])]\n",
    "\n",
    "        # Skip rows that don't have enough columns\n",
    "        if len(cols) < 5:\n",
    "            continue\n",
    "\n",
    "        # Remove footnotes from country name\n",
    "        import re\n",
    "        country_name_clean = re.sub(r\"\\[.*?\\]\", \"\", cols[1]).strip()\n",
    "\n",
    "        country_info = {\n",
    "            \"Rank\": cols[0],\n",
    "            \"Country\": country_name_clean,  # cleaned from footnotes name\n",
    "            \"Population\": cols[2],\n",
    "            \"Date\": cols[3],\n",
    "            \"Source\": cols[4],\n",
    "        }\n",
    "        data.append(country_info)\n",
    "\n",
    "    return data\n",
    "\n",
    "\n",
    "# Run scraping\n",
    "url = \"https://en.wikipedia.org/wiki/List_of_countries_and_dependencies_by_population_(United_Nations)\"\n",
    "html = fetch_html(url)\n",
    "countries = parse_countries_page(html)\n",
    "\n",
    "print(f\"Extracted {len(countries)} countries\")\n",
    "for c in countries[:5]:\n",
    "    print(c)\n"
   ],
   "id": "47726016150dc060",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracted 238 countries\n",
      "{'Rank': 'World', 'Country': '8,021,407,192', 'Population': '8,091,734,930', 'Date': '+0.88%', 'Source': '–'}\n",
      "{'Rank': 'India', 'Country': '1,425,423,212', 'Population': '1,438,069,596', 'Date': '+0.89%', 'Source': 'Asia'}\n",
      "{'Rank': 'China[a]', 'Country': '1,425,179,569', 'Population': '1,422,584,933', 'Date': '−0.18%', 'Source': 'Asia'}\n",
      "{'Rank': 'United States', 'Country': '341,534,046', 'Population': '343,477,335', 'Date': '+0.57%', 'Source': 'Americas'}\n",
      "{'Rank': 'Indonesia', 'Country': '278,830,529', 'Population': '281,190,067', 'Date': '+0.85%', 'Source': 'Asia'}\n"
     ]
    }
   ],
   "execution_count": 77
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-24T09:17:03.802683Z",
     "start_time": "2025-10-24T09:17:03.800197Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Pagination and scraping\n",
    "from typing import Optional, List, Dict, Any\n",
    "\n",
    "# Since Wikipedia's population list is on a single page, there is no pagination\n",
    "def find_next_page_url(html: str, base_url: str) -> Optional[str]:\n",
    "\n",
    "    return None\n",
    "\n",
    "\n",
    "# Main scraping function that could handle multiple pages\n",
    "def scrape_all_countries(start_url: str, max_pages: int = 1) -> List[Dict[str, Any]]:\n",
    "\n",
    "    all_rows: List[Dict[str, Any]] = []\n",
    "    url: Optional[str] = start_url\n",
    "    pages_visited: int = 0\n",
    "\n",
    "    while url and pages_visited < max_pages:\n",
    "        # Fetch HTML from the current URL\n",
    "        html: str = fetch_html(url)\n",
    "\n",
    "        # Parse the table and extract country data\n",
    "        page_rows: List[Dict[str, Any]] = parse_countries_page(html)\n",
    "        all_rows.extend(page_rows)\n",
    "\n",
    "        # Wikipedia does not paginate, so this will always return None\n",
    "        url = find_next_page_url(html, base_url=url)\n",
    "        pages_visited += 1\n",
    "\n",
    "    return all_rows\n"
   ],
   "id": "ae70c4a5fbddfb3",
   "outputs": [],
   "execution_count": 78
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-24T09:17:05.556704Z",
     "start_time": "2025-10-24T09:17:03.812614Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# CSV export\n",
    "from __future__ import annotations\n",
    "from typing import List, Dict, Any\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "# Convert list of country dicts to DataFrame\n",
    "def countries_to_dataframe(rows: List[Dict[str, Any]]) -> pd.DataFrame:\n",
    "\n",
    "    return pd.DataFrame.from_records(rows, columns=[\"Rank\", \"Country\", \"Population\", \"Date\", \"Source\"])\n",
    "\n",
    "\n",
    "# Save countries DataFrame to CSV with ; separator\n",
    "def save_countries_csv(rows: List[Dict[str, Any]], path: str) -> None:\n",
    "    df = countries_to_dataframe(rows)\n",
    "    df.to_csv(path, sep=\";\", index=False)\n",
    "\n",
    "\n",
    "# Run scraping and save to CSV\n",
    "try:\n",
    "    countries = scrape_all_countries(\n",
    "        \"https://en.wikipedia.org/wiki/List_of_countries_and_dependencies_by_population_(United_Nations)\"\n",
    "    )\n",
    "    save_path = \"countries_population.csv\"\n",
    "    save_countries_csv(countries, save_path)\n",
    "    print(f\"Saved {len(countries)} countries to {save_path}\")\n",
    "    display(countries_to_dataframe(countries).head(5))\n",
    "except Exception as e:\n",
    "    print(f\"Save failed: {e}\")\n"
   ],
   "id": "fd26192283df997c",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved 238 countries to countries_population.csv\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "            Rank        Country     Population    Date    Source\n",
       "0          World  8,021,407,192  8,091,734,930  +0.88%         –\n",
       "1          India  1,425,423,212  1,438,069,596  +0.89%      Asia\n",
       "2       China[a]  1,425,179,569  1,422,584,933  −0.18%      Asia\n",
       "3  United States    341,534,046    343,477,335  +0.57%  Americas\n",
       "4      Indonesia    278,830,529    281,190,067  +0.85%      Asia"
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Rank</th>\n",
       "      <th>Country</th>\n",
       "      <th>Population</th>\n",
       "      <th>Date</th>\n",
       "      <th>Source</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>World</td>\n",
       "      <td>8,021,407,192</td>\n",
       "      <td>8,091,734,930</td>\n",
       "      <td>+0.88%</td>\n",
       "      <td>–</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>India</td>\n",
       "      <td>1,425,423,212</td>\n",
       "      <td>1,438,069,596</td>\n",
       "      <td>+0.89%</td>\n",
       "      <td>Asia</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>China[a]</td>\n",
       "      <td>1,425,179,569</td>\n",
       "      <td>1,422,584,933</td>\n",
       "      <td>−0.18%</td>\n",
       "      <td>Asia</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>United States</td>\n",
       "      <td>341,534,046</td>\n",
       "      <td>343,477,335</td>\n",
       "      <td>+0.57%</td>\n",
       "      <td>Americas</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Indonesia</td>\n",
       "      <td>278,830,529</td>\n",
       "      <td>281,190,067</td>\n",
       "      <td>+0.85%</td>\n",
       "      <td>Asia</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data",
     "jetTransient": {
      "display_id": null
     }
    }
   ],
   "execution_count": 79
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Documentation\n",
    "\n",
    "**Target:**\n",
    "I've scraped the Wikipedia page *\"List of countries and dependencies by population (United Nations)\"* because it contains a well-structured table of countries with population data, suitable for educational web scraping without login or authentication.\n",
    "\n",
    "**Previous attempts:**\n",
    "I initially tried scraping three websites: two educational sandboxes and one regular site. These attempts failed because the sites were dynamic and relied heavily on JavaScript, making it impossible to fetch the data directly from the HTML.\n",
    "\n",
    "**What worked:**\n",
    "- Fetched the HTML successfully using `fetch_html()`.\n",
    "- Parsed the main table and extracted fields: **Rank**, **Country**, **Population**, **Date**, and **Source**.\n",
    "- Converted the data into a Pandas DataFrame and exported it to CSV using `;` as the separator.\n",
    "- Maintained polite scraping with `time.sleep()` between requests.\n",
    "\n",
    "**Challenges:**\n",
    "- Wikipedia does not use pagination, so `find_next_page_url()` always returns `None`.\n",
    "- Some country names include footnotes (e.g., `[a]`), which initially appeared in the scraped text.\n",
    "\n",
    "**How we handled it:**\n",
    "- `scrape_all_countries()` was designed to handle multiple pages, but for Wikipedia it collects all data from the single page.\n",
    "- Footnotes were removed from country names using a regular expression during parsing.\n"
   ],
   "id": "e4f7158b735ebb64"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
